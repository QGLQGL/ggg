{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install keras-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n"
    },
    {
     "data": {
      "text/plain": "'1.14.0'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_1 (Flatten)          (None, 4)                 0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                80        \n_________________________________________________________________\nactivation_1 (Activation)    (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 34        \n_________________________________________________________________\nactivation_2 (Activation)    (None, 2)                 0         \n=================================================================\nTotal params: 114\nTrainable params: 114\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.000, 1.000], mean observation: 0.117 [-1.805, 2.614], loss: 1.452632, mae: 5.160711, mean_q: 9.613422\n 2415/5000: episode: 232, duration: 0.230s, episode steps: 9, steps per second: 39, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.170 [-1.345, 2.323], loss: 1.546928, mae: 5.115371, mean_q: 9.510553\n 2428/5000: episode: 233, duration: 0.266s, episode steps: 13, steps per second: 49, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.079 [-1.797, 2.652], loss: 2.014740, mae: 5.108748, mean_q: 9.382926\n 2438/5000: episode: 234, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.137 [-1.188, 2.046], loss: 1.331639, mae: 5.055463, mean_q: 9.469430\n 2448/5000: episode: 235, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.148 [-1.741, 2.717], loss: 1.577264, mae: 5.108215, mean_q: 9.470144\n 2457/5000: episode: 236, duration: 0.212s, episode steps: 9, steps per second: 42, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.145 [-1.615, 2.499], loss: 1.255955, mae: 4.983869, mean_q: 9.311037\n 2466/5000: episode: 237, duration: 0.202s, episode steps: 9, steps per second: 45, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.162 [-1.539, 2.506], loss: 1.420223, mae: 5.171848, mean_q: 9.643955\n 2476/5000: episode: 238, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.117 [-1.030, 1.736], loss: 1.373312, mae: 5.000320, mean_q: 9.352806\n 2487/5000: episode: 239, duration: 0.216s, episode steps: 11, steps per second: 51, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.113 [-1.406, 2.265], loss: 1.670473, mae: 5.062593, mean_q: 9.317261\n 2499/5000: episode: 240, duration: 0.231s, episode steps: 12, steps per second: 52, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.558, 2.351], loss: 1.359767, mae: 4.977615, mean_q: 9.261318\n 2510/5000: episode: 241, duration: 0.242s, episode steps: 11, steps per second: 45, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.136 [-1.379, 2.241], loss: 1.121750, mae: 5.052203, mean_q: 9.646588\n 2520/5000: episode: 242, duration: 0.229s, episode steps: 10, steps per second: 44, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.110 [-1.610, 2.384], loss: 1.422534, mae: 5.174077, mean_q: 9.698916\n 2530/5000: episode: 243, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.166 [-1.718, 2.747], loss: 1.133573, mae: 5.041868, mean_q: 9.511270\n 2539/5000: episode: 244, duration: 0.192s, episode steps: 9, steps per second: 47, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.108 [-1.406, 2.118], loss: 0.958462, mae: 4.982757, mean_q: 9.469540\n 2549/5000: episode: 245, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.120 [-1.618, 2.532], loss: 1.774519, mae: 5.116163, mean_q: 9.494726\n 2558/5000: episode: 246, duration: 0.192s, episode steps: 9, steps per second: 47, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.606, 2.513], loss: 1.072925, mae: 4.978666, mean_q: 9.461001\n 2568/5000: episode: 247, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.129 [-1.540, 2.392], loss: 0.995400, mae: 4.993444, mean_q: 9.482564\n 2577/5000: episode: 248, duration: 0.183s, episode steps: 9, steps per second: 49, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.132 [-1.415, 2.286], loss: 1.090050, mae: 5.024905, mean_q: 9.452389\n 2588/5000: episode: 249, duration: 0.219s, episode steps: 11, steps per second: 50, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.098 [-1.598, 2.323], loss: 1.271122, mae: 4.922568, mean_q: 9.226864\n 2599/5000: episode: 250, duration: 0.249s, episode steps: 11, steps per second: 44, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.130 [-1.540, 2.339], loss: 1.281906, mae: 5.032094, mean_q: 9.417437\n 2607/5000: episode: 251, duration: 0.169s, episode steps: 8, steps per second: 47, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.557, 2.562], loss: 1.545981, mae: 4.943573, mean_q: 9.145512\n 2619/5000: episode: 252, duration: 0.267s, episode steps: 12, steps per second: 45, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.097 [-1.180, 1.834], loss: 0.954648, mae: 4.903147, mean_q: 9.241200\n 2628/5000: episode: 253, duration: 0.159s, episode steps: 9, steps per second: 56, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.177 [-1.519, 2.523], loss: 1.078905, mae: 4.923235, mean_q: 9.201282\n 2636/5000: episode: 254, duration: 0.141s, episode steps: 8, steps per second: 57, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.574, 2.565], loss: 1.035129, mae: 5.020038, mean_q: 9.502384\n 2646/5000: episode: 255, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.129 [-1.335, 2.124], loss: 1.037405, mae: 4.833029, mean_q: 9.059584\n 2660/5000: episode: 256, duration: 0.274s, episode steps: 14, steps per second: 51, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.084 [-1.335, 2.079], loss: 1.211751, mae: 4.975172, mean_q: 9.351480\n 2668/5000: episode: 257, duration: 0.157s, episode steps: 8, steps per second: 51, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.157 [-1.336, 2.245], loss: 1.365978, mae: 4.897136, mean_q: 9.154555\n 2677/5000: episode: 258, duration: 0.177s, episode steps: 9, steps per second: 51, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.140 [-1.354, 2.247], loss: 0.966510, mae: 4.896750, mean_q: 9.229981\n 2688/5000: episode: 259, duration: 0.185s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-1.186, 1.923], loss: 0.996050, mae: 4.958682, mean_q: 9.353135\n 2699/5000: episode: 260, duration: 0.202s, episode steps: 11, steps per second: 54, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.105 [-1.190, 1.816], loss: 0.918914, mae: 4.966327, mean_q: 9.432659\n 2708/5000: episode: 261, duration: 0.195s, episode steps: 9, steps per second: 46, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.137 [-1.191, 1.887], loss: 1.100214, mae: 4.894791, mean_q: 9.238071\n 2735/5000: episode: 262, duration: 0.566s, episode steps: 27, steps per second: 48, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.137 [-1.004, 2.044], loss: 0.782033, mae: 4.848526, mean_q: 9.196574\n 2745/5000: episode: 263, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-0.984, 1.561], loss: 1.618186, mae: 4.974032, mean_q: 9.245859\n 2757/5000: episode: 264, duration: 0.227s, episode steps: 12, steps per second: 53, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.093 [-1.190, 1.720], loss: 0.957419, mae: 4.832490, mean_q: 9.022170\n 2766/5000: episode: 265, duration: 0.197s, episode steps: 9, steps per second: 46, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.135 [-1.227, 1.936], loss: 0.998786, mae: 4.871216, mean_q: 9.156341\n 2777/5000: episode: 266, duration: 0.197s, episode steps: 11, steps per second: 56, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.106 [-1.167, 1.758], loss: 0.833833, mae: 4.927414, mean_q: 9.315096\n 2813/5000: episode: 267, duration: 0.704s, episode steps: 36, steps per second: 51, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.150 [-0.971, 0.985], loss: 1.023227, mae: 4.788499, mean_q: 8.973104\n 2882/5000: episode: 268, duration: 1.414s, episode steps: 69, steps per second: 49, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.073 [-1.025, 1.253], loss: 1.024793, mae: 4.823490, mean_q: 9.017538\n 2934/5000: episode: 269, duration: 0.955s, episode steps: 52, steps per second: 54, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.118 [-0.936, 1.007], loss: 0.895546, mae: 4.825182, mean_q: 9.060258\n 2963/5000: episode: 270, duration: 0.567s, episode steps: 29, steps per second: 51, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.127 [-0.784, 1.737], loss: 0.952397, mae: 4.916917, mean_q: 9.241443\n 2999/5000: episode: 271, duration: 0.701s, episode steps: 36, steps per second: 51, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.098 [-1.232, 0.239], loss: 0.723265, mae: 4.909304, mean_q: 9.265834\n 3051/5000: episode: 272, duration: 0.900s, episode steps: 52, steps per second: 58, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.112 [-0.830, 0.605], loss: 1.008629, mae: 4.922679, mean_q: 9.270090\n 3136/5000: episode: 273, duration: 1.516s, episode steps: 85, steps per second: 56, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.070 [-0.834, 0.546], loss: 0.796096, mae: 5.007464, mean_q: 9.479342\n 3175/5000: episode: 274, duration: 0.683s, episode steps: 39, steps per second: 57, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.127 [-0.690, 0.175], loss: 0.904179, mae: 5.053207, mean_q: 9.518229\n 3242/5000: episode: 275, duration: 1.146s, episode steps: 67, steps per second: 58, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.118 [-0.483, 0.981], loss: 1.081691, mae: 5.137798, mean_q: 9.641620\n 3317/5000: episode: 276, duration: 1.331s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.072 [-0.665, 0.393], loss: 0.918846, mae: 5.198277, mean_q: 9.804819\n 3381/5000: episode: 277, duration: 1.106s, episode steps: 64, steps per second: 58, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.075 [-0.701, 0.353], loss: 0.850423, mae: 5.250274, mean_q: 9.912426\n 3449/5000: episode: 278, duration: 1.252s, episode steps: 68, steps per second: 54, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.105 [-0.563, 0.912], loss: 0.973256, mae: 5.360178, mean_q: 10.082127\n 3497/5000: episode: 279, duration: 0.839s, episode steps: 48, steps per second: 57, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.053 [-0.850, 0.403], loss: 0.963154, mae: 5.382051, mean_q: 10.103989\n 3545/5000: episode: 280, duration: 0.823s, episode steps: 48, steps per second: 58, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.096 [-0.863, 0.600], loss: 1.075196, mae: 5.469288, mean_q: 10.266650\n 3585/5000: episode: 281, duration: 0.707s, episode steps: 40, steps per second: 57, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.125 [-0.831, 0.292], loss: 1.071554, mae: 5.480945, mean_q: 10.310788\n 3707/5000: episode: 282, duration: 2.184s, episode steps: 122, steps per second: 56, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.023 [-0.923, 0.612], loss: 1.032171, mae: 5.594934, mean_q: 10.537616\n 3757/5000: episode: 283, duration: 0.845s, episode steps: 50, steps per second: 59, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.123 [-0.250, 0.866], loss: 1.285522, mae: 5.668962, mean_q: 10.620399\n 3780/5000: episode: 284, duration: 0.479s, episode steps: 23, steps per second: 48, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.100 [-0.734, 0.382], loss: 1.019615, mae: 5.839321, mean_q: 11.036785\n 3819/5000: episode: 285, duration: 0.652s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.125 [-0.684, 0.262], loss: 1.004864, mae: 5.753545, mean_q: 10.830783\n 3883/5000: episode: 286, duration: 1.075s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.087 [-0.758, 0.356], loss: 1.138979, mae: 5.893931, mean_q: 11.111090\n 3940/5000: episode: 287, duration: 0.994s, episode steps: 57, steps per second: 57, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.089 [-0.265, 0.852], loss: 1.289278, mae: 5.950086, mean_q: 11.190371\n 3990/5000: episode: 288, duration: 0.904s, episode steps: 50, steps per second: 55, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.102 [-0.378, 0.788], loss: 1.006631, mae: 6.011626, mean_q: 11.359350\n 4028/5000: episode: 289, duration: 0.656s, episode steps: 38, steps per second: 58, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.107 [-0.391, 1.229], loss: 1.352111, mae: 6.137953, mean_q: 11.526228\n 4063/5000: episode: 290, duration: 0.620s, episode steps: 35, steps per second: 56, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.094 [-0.775, 0.387], loss: 1.059906, mae: 6.250526, mean_q: 11.816990\n 4138/5000: episode: 291, duration: 1.299s, episode steps: 75, steps per second: 58, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.042 [-0.739, 0.246], loss: 1.289285, mae: 6.262785, mean_q: 11.807910\n 4161/5000: episode: 292, duration: 0.391s, episode steps: 23, steps per second: 59, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.135 [-0.707, 0.375], loss: 1.187392, mae: 6.171266, mean_q: 11.602078\n 4201/5000: episode: 293, duration: 0.744s, episode steps: 40, steps per second: 54, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.107 [-0.692, 0.423], loss: 1.559549, mae: 6.339240, mean_q: 11.870646\n 4263/5000: episode: 294, duration: 1.084s, episode steps: 62, steps per second: 57, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.090 [-1.046, 0.757], loss: 1.436399, mae: 6.318448, mean_q: 11.883467\n 4304/5000: episode: 295, duration: 0.716s, episode steps: 41, steps per second: 57, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.139 [-0.621, 0.205], loss: 1.223595, mae: 6.518849, mean_q: 12.320799\n 4357/5000: episode: 296, duration: 1.002s, episode steps: 53, steps per second: 53, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.116 [-0.672, 0.401], loss: 1.267278, mae: 6.498923, mean_q: 12.246397\n 4427/5000: episode: 297, duration: 1.200s, episode steps: 70, steps per second: 58, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.079 [-1.012, 0.403], loss: 1.358260, mae: 6.605415, mean_q: 12.473358\n 4546/5000: episode: 298, duration: 2.018s, episode steps: 119, steps per second: 59, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.320 [-0.504, 1.497], loss: 1.366528, mae: 6.655883, mean_q: 12.538073\n 4598/5000: episode: 299, duration: 0.927s, episode steps: 52, steps per second: 56, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.068 [-0.754, 0.425], loss: 1.327363, mae: 6.778399, mean_q: 12.795712\n 4655/5000: episode: 300, duration: 1.021s, episode steps: 57, steps per second: 56, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.123 [-0.871, 0.430], loss: 1.759409, mae: 6.933169, mean_q: 13.049173\n 4725/5000: episode: 301, duration: 1.365s, episode steps: 70, steps per second: 51, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.092 [-0.679, 0.440], loss: 1.525672, mae: 6.945333, mean_q: 13.110428\n 4787/5000: episode: 302, duration: 1.058s, episode steps: 62, steps per second: 59, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.163 [-0.924, 0.400], loss: 1.488724, mae: 7.045378, mean_q: 13.323568\n 4837/5000: episode: 303, duration: 0.934s, episode steps: 50, steps per second: 54, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.166 [-0.775, 0.248], loss: 1.216027, mae: 7.199146, mean_q: 13.680140\n 4880/5000: episode: 304, duration: 0.749s, episode steps: 43, steps per second: 57, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.089 [-0.612, 0.240], loss: 1.686047, mae: 7.156516, mean_q: 13.540504\n 4921/5000: episode: 305, duration: 0.762s, episode steps: 41, steps per second: 54, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.101 [-0.667, 0.375], loss: 1.829235, mae: 7.254486, mean_q: 13.675291\ndone, took 110.830 seconds\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1f0f53aed88>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Testing for 5 episodes ...\nEpisode 1: reward: 87.000, steps: 87\nEpisode 2: reward: 50.000, steps: 50\nEpisode 3: reward: 200.000, steps: 200\nEpisode 4: reward: 47.000, steps: 47\nEpisode 5: reward: 103.000, steps: 103\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1f0f40ff3c8>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              (None, 16)                80        \n_________________________________________________________________\nactivation_3 (Activation)    (None, 16)                0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 2)                 34        \n_________________________________________________________________\nactivation_4 (Activation)    (None, 2)                 0         \n=================================================================\nTotal params: 114\nTrainable params: 114\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_dim=env.observation_space.shape[0]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(4,)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}